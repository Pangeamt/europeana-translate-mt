{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from nltk.metrics.scores import f_measure\n",
    "import numpy as np\n",
    "import torchmetrics as torchm\n",
    "import nltk \n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer = torchm.WordErrorRate()\n",
    "cer = torchm.CharErrorRate()\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def sets(refs, preds):\n",
    "    refs_set = set()\n",
    "    preds_set = set()\n",
    "    r_words = tokenizer.tokenize(refs.lower())\n",
    "    for w_r in r_words:\n",
    "        if w_r not in refs_set:\n",
    "            refs_set.add(w_r)\n",
    "    p_words = tokenizer.tokenize(preds.lower())\n",
    "    for w_p in p_words:\n",
    "        if w_p not in preds_set:\n",
    "            preds_set.add(w_p)\n",
    "    \n",
    "\n",
    "    return refs_set, preds_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input('Introduce the translation training data: '), sep = '\\t')\n",
    "df = df[['source','translation','evaluator1 scores', 'evaluator2 scores']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_list, cer_list, f_list = [], [], []\n",
    "n = 0\n",
    "for src, tgt in zip(df['source'], df['translation']):\n",
    "#     print(n)\n",
    "    a = time.time()\n",
    "#     if n == 100:\n",
    "#         break\n",
    "    print(len(src),len(tgt))\n",
    "    wer_score = wer(target=src, preds=tgt).item() * 100\n",
    "    wer_list.append(wer_score)\n",
    "    cer_score = cer(target=src, preds=tgt).item() * 100\n",
    "    cer_list.append(cer_score)\n",
    "    r_s, p_s = sets(src, tgt)\n",
    "    f_score = f_measure(r_s, p_s) * 100\n",
    "    f_list.append(f_score)\n",
    "    n+=1\n",
    "    print(time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wer'] = wer_list\n",
    "df['cer'] = cer_list\n",
    "df['f_score'] = f_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[['wer','cer','f_score']]\n",
    "y = df['evaluator scores']\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)\n",
    "regr.fit(x_, y)\n",
    "\n",
    "predictions = regr.predict(input('Introduce the translation test data: ')) \n",
    "\n",
    "print('Pearson corr: ',stats.pearsonr(df['evaluator scores'],predictions)[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
