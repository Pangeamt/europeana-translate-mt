{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from nltk.metrics.scores import f_measure\n",
    "import numpy as np\n",
    "import torchmetrics as torchm\n",
    "import nltk \n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "wer = torchm.WordErrorRate()  # Initializing WordErrorRate object for word error rate computation\n",
    "cer = torchm.CharErrorRate()  # Initializing CharErrorRate object for character error rate computation\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')  # Initializing a RegexpTokenizer object to tokenize text based on word boundaries\n",
    "\n",
    "# Function definition to create sets of unique words from reference and prediction strings\n",
    "def sets(refs, preds):\n",
    "    refs_set = set()  # Set to store unique words from reference string\n",
    "    preds_set = set()  # Set to store unique words from prediction string\n",
    "    \n",
    "    # Tokenizing reference string and converting to lowercase\n",
    "    r_words = tokenizer.tokenize(refs.lower())  \n",
    "    # Iterating through each word in reference string\n",
    "    for w_r in r_words:\n",
    "        # Adding each word to the set if not already present\n",
    "        if w_r not in refs_set:\n",
    "            refs_set.add(w_r)  \n",
    "    \n",
    "    # Tokenizing prediction string and converting to lowercase\n",
    "    p_words = tokenizer.tokenize(preds.lower())  \n",
    "    # Iterating through each word in prediction string\n",
    "    for w_p in p_words:\n",
    "        # Adding each word to the set if not already present\n",
    "        if w_p not in preds_set:\n",
    "            preds_set.add(w_p)  \n",
    "    \n",
    "    # Returning sets of unique words from reference and prediction strings\n",
    "    return refs_set, preds_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the translation training data from user input and storing it in a DataFrame\n",
    "# The data is assumed to be in tab-separated format, hence the separator '\\t'\n",
    "df = pd.read_csv(input('Introduce the translation training data: '), sep='\\t')\n",
    "\n",
    "# Selecting specific columns from the DataFrame for further processing\n",
    "# Assuming 'source' column contains original text, 'translation' column contains translated text,\n",
    "# and 'evaluator1 scores' and 'evaluator2 scores' contain scores given by two different evaluators\n",
    "df = df[['source', 'translation', 'evaluator1 scores', 'evaluator2 scores']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing empty lists to store Word Error Rate (WER), Character Error Rate (CER), and F-measure scores\n",
    "wer_list, cer_list, f_list = [], [], []\n",
    "\n",
    "# Initializing a counter variable\n",
    "n = 0\n",
    "\n",
    "# Iterating through each pair of source and translation texts in the DataFrame\n",
    "for src, tgt in zip(df['source'], df['translation']):\n",
    "    # Start measuring time\n",
    "    a = time.time()\n",
    "    \n",
    "    # Printing the lengths of source and target texts\n",
    "    print(len(src), len(tgt))\n",
    "    \n",
    "    # Computing the Word Error Rate (WER) score between source and target texts and converting to percentage\n",
    "    wer_score = wer(target=src, preds=tgt).item() * 100\n",
    "    # Appending the WER score to the WER list\n",
    "    wer_list.append(wer_score)\n",
    "    \n",
    "    # Computing the Character Error Rate (CER) score between source and target texts and converting to percentage\n",
    "    cer_score = cer(target=src, preds=tgt).item() * 100\n",
    "    # Appending the CER score to the CER list\n",
    "    cer_list.append(cer_score)\n",
    "    \n",
    "    # Creating sets of unique words from source and target texts\n",
    "    r_s, p_s = sets(src, tgt)\n",
    "    # Computing the F-measure score between the sets of unique words from source and target texts and converting to percentage\n",
    "    f_score = f_measure(r_s, p_s) * 100\n",
    "    # Appending the F-measure score to the F-measure list\n",
    "    f_list.append(f_score)\n",
    "    \n",
    "    # Incrementing the counter variable\n",
    "    n += 1\n",
    "    \n",
    "    # Printing the time taken for processing the current pair of texts\n",
    "    print(time.time() - a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scores to the data frame\n",
    "df['wer'] = wer_list\n",
    "df['cer'] = cer_list\n",
    "df['f_score'] = f_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns 'wer', 'cer', and 'f_score' from the DataFrame and assigning them to variable x\n",
    "x = df[['wer', 'cer', 'f_score']]\n",
    "\n",
    "# Selecting 'evaluator scores' column from the DataFrame and assigning it to variable y\n",
    "y = df['evaluator scores']\n",
    "\n",
    "# Initializing a linear regression model\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Transforming the features using PolynomialFeatures to create quadratic features without including bias\n",
    "x_ = PolynomialFeatures(degree=2, include_bias=False).fit_transform(x)\n",
    "\n",
    "# Fitting the transformed features and the target values to the linear regression model\n",
    "regr.fit(x_, y)\n",
    "\n",
    "# Predicting evaluator scores for translation test data provided by the user\n",
    "predictions = regr.predict(input('Introduce the translation test data: '))\n",
    "\n",
    "# Computing Pearson correlation coefficient between the actual evaluator scores and predicted scores\n",
    "pearson_corr = stats.pearsonr(df['evaluator scores'], predictions)[0]\n",
    "\n",
    "# Printing the Pearson correlation coefficient\n",
    "print('Pearson corr: ', pearson_corr)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
